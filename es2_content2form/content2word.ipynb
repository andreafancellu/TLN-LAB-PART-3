{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Esercizio 2 - Content2Word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'obiettivo di questo erercizio è, date le definizioni presenti nel file *def.csv* usate per l'esercizio 1, risalire al termine che le descrive. Si tratta quindi di una ***ricerca onomasiologica***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approccio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Innanzitutto vengono presi i termini più frequenti nelle definizioni, che saranno i ***genus***, usati per restringere la ricerca su *WordNet*.\n",
    "\n",
    "- ***Text cleaning***: rimozione di *stopwords* e *lemmatizzazione*.\n",
    "- Prelievo di tutto il sottoalbero di iponimi dei genus e salvataggio delle loro definizioni (glosse).\n",
    "- Confronto tra le definizioni di *Wordnet* e la nostra lista di definizioni. Il termine scelto è quello che massimizza l'*overlap* tra la definizione di *Wordnet* e la lista di definizioni in *def.csv*.\n",
    "- Infine viene restituito il synset che ha la definizione più simile a quella della lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from gensim.test.utils import simple_preprocess\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet as wn\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funzioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_from_file(path): # rimuove le stopwords e restituisce lista di parole nel file in path\n",
    "    file = []\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    with open (path, 'r') as f:\n",
    "        for row in f:\n",
    "            filtered_s = [w for w in word_tokenize(row) if not w.lower() in stop_words]\n",
    "            file.append(simple_preprocess(str(filtered_s), deacc=True))\n",
    "    f.close()\n",
    "    return file\n",
    "\n",
    "def get_most_freq_words(text, nword): # restituisce le n parole più frequenti in text, per ogni riga del documento\n",
    "    genus = []\n",
    "    for row in text:\n",
    "        c = Counter()\n",
    "        c.update(row)\n",
    "        genus.append(c.most_common(nword))\n",
    "    return genus\n",
    "\n",
    "def get_hypos(word): # restituisce tutti gli iponimi di word\n",
    "    syn = get_synset(word)\n",
    "    hypo_list = []\n",
    "    if(syn is not None):\n",
    "        hypo_list = list(set([w for s in syn.closure(lambda s:s.hyponyms()) for w in s.lemma_names()]))\n",
    "    return hypo_list\n",
    "\n",
    "def get_hypers(word): # restituisce tutti gli iperonimi di word\n",
    "    syn = get_synset(word)\n",
    "    hyper_list = []\n",
    "    if(syn is not None):\n",
    "        hyper_list = list(set([w for s in syn.closure(lambda s:s.hypermyms()) for w in s.lemma_names()]))\n",
    "    return hyper_list\n",
    "\n",
    "def get_synset(word): # restituisce il primo synset di word\n",
    "    if(len(wn.synsets(word)) > 0):\n",
    "        return wn.synsets(word)[0]\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Esecuzione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_genus = 3 \n",
    "num_most_freq_word = 10 \n",
    "starting_words = [\"Emotion\", \"Person\", \"Revenge\", \"Brick\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('feeling', 11), ('human', 8), ('feel', 8)], [('human', 26), ('person', 5), ('homo', 5)], [('someone', 14), ('feeling', 7), ('anger', 7)], [('used', 22), ('object', 15), ('material', 13)]]\n"
     ]
    }
   ],
   "source": [
    "file = get_text_from_file('../data/def.csv')\n",
    "genus = get_most_freq_words(file, num_genus)\n",
    "print (genus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Word: Emotion /  Genus --> ('feeling', 11)\n",
      "Word: *affect*, score: *3*\n",
      "Word: *eagerness*, score: *3*\n",
      "Word: *ambivalency*, score: *3*\n",
      "Word: *daze*, score: *3*\n",
      "Word: *discomfort*, score: *3*\n",
      "\n",
      "Original Word: Person /  Genus --> ('human', 26)\n",
      "Word: *Zuni*, score: *2*\n",
      "Word: *Algonquian*, score: *2*\n",
      "Word: *Elamite*, score: *2*\n",
      "Word: *Cocopa*, score: *2*\n",
      "Word: *Kanarese*, score: *2*\n",
      "\n",
      "Original Word: Revenge /  Genus --> ('someone', 14)\n",
      "Word: *sounding_board*, score: *5*\n",
      "Word: *impassiveness*, score: *4*\n",
      "Word: *unemotionality*, score: *4*\n",
      "Word: *emotionlessness*, score: *4*\n",
      "Word: *hatred*, score: *4*\n",
      "\n",
      "Original Word: Brick /  Genus --> ('used', 22)\n",
      "Word: *brick*, score: *5*\n",
      "Word: *building_material*, score: *4*\n",
      "Word: *marble*, score: *3*\n",
      "Word: *kaolin*, score: *3*\n",
      "Word: *Plasticine*, score: *3*\n"
     ]
    }
   ],
   "source": [
    "key_words_defs = get_most_freq_words(file, num_most_freq_word) # 10 parole più frequenti per ogni riga del documento\n",
    "\n",
    "for i in range(len(genus)):\n",
    "    \n",
    "    # collasso key_words_defs in un unico array\n",
    "    key_row = []\n",
    "    for el in key_words_defs[i]:\n",
    "        key_row.append(el[0])\n",
    "\n",
    "    # salvo gli iponimi di ogni genus\n",
    "    hypo_list, hypo_def = [], []\n",
    "    for el in genus[i]:\n",
    "        hypo_list.append(get_hypos(el[0])) \n",
    "     \n",
    "    hypo_list = [x for xs in hypo_list for x in xs]\n",
    "\n",
    "    # definizioni di ogni iponimo\n",
    "    for hypo in hypo_list:\n",
    "        hypo_def.append((hypo, get_synset(hypo).definition()))\n",
    "    \n",
    "    # salvo parole che massimizzano l'overlap e i punteggi (numero di intersezioni)\n",
    "    res = []\n",
    "    for wndef in hypo_def: # definizioni di ogni iponimo \n",
    "        score = 0\n",
    "        imp_words = []\n",
    "        for key_word in key_row: # definizioni in def.csv\n",
    "            if(key_word in wndef[1]):\n",
    "                score += 1\n",
    "                imp_words.append(key_word)      \n",
    "\n",
    "        res.append((score, wndef[0], imp_words, wndef[1]))\n",
    "        \n",
    "    # ordino sullo score e inverto\n",
    "    sorted_list = sorted(res, key=lambda x: x[0]) \n",
    "    sorted_res = list(reversed(sorted_list))\n",
    "    \n",
    "    print(\"\\nOriginal Word:\", starting_words[i], \"/  Genus -->\",genus[i][0])\n",
    "    for k in range(min(len(sorted_res), 5)):\n",
    "        # print(f'Word: *{sorted_res[k][1]}*, score: *{sorted_res[k][0]}*, the key words are *{sorted_res[k][2]}* and the definition is *{sorted_res[k][3]}*')\n",
    "        print(f'Word: *{sorted_res[k][1]}*, score: *{sorted_res[k][0]}*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analisi dei risultati"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alcuni problemi riscontrati:\n",
    "\n",
    "- **I genus**: i genus di riferimento hanno poco a che vedere con i termini, come ad esempio *\"someone\"* per *revenge*. Questo è portato dal dataset in input. Si potrebbe ripulire il dataset rimuovendo le parole che non hanno a che vedere con il termine originale, ma così facendo si andrebbe a compromettere l'esercizio e si renderebbe questo approccio poco scalabile su altre basi di dati.\n",
    "   \n",
    "- **Iponimi**: non è detto che il termine ricercato sia un iponimo dei genus che selezioanti, infatti potrebbe essere \n",
    "   un iperonimo o essere proprio in un punto completamente diverso dell'albero di *Wordnet*. Un possibile miglioramento dell'algoritmo potrebbe\n",
    "   essere quello di andare a prelevare altri synset oltre agli iponimi del genus, ad esempio andando a prelevare anche i fratelli del genus,\n",
    "   senza allontarsi troppo per non far esplodere la complessità dell'algoritmo.\n",
    "\n",
    "- **Funzione di similarità**: l'algoritmo è basato su uno *score* che corrisponde a quante parole simili ci sono nelle definizioni. Si potrebbe \n",
    "   incrementare lo *score* sulla base di altri fattori, come la funzione di similarità di wordnet *path_similarity*.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6847c98a8f86b01c6a19c518cd2f366693b80566b266804d5ca763cbb223f52b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
