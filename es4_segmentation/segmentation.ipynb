{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Esercizio 4 - Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approccio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'algoritmo utilizza una tecnica *brute force* per trovare i tagli corretti. Prova diverse combinazioni di tagli generandoli in maniera randomica e associa ad ognuna un punteggio. Il punteggio è calcolato come la somma degli *n* termini più frequnti all'interno di ogni segmento.\n",
    "\n",
    "L'idea alla base è che per ogni argomento ci siano un numero di parole che occorrono molte volte, e quindi l'algoritmo andrà a cercare \n",
    "i tagli che vanno a massimizzare questi valori, e che quindi vanno ad isolare i termini più frequenti per ogni segmento.\n",
    "\n",
    "L'algoritmo ha bisogno del numero di tagli in input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*data/segmentation_eng*: Il dataset è costituito da pezzi dei paragrafi di wikipedia di 4 argomenti diversi, in questo caso i 4 argomenti sono:\n",
    "  - Gorilla\n",
    "  - Quantum Computing\n",
    "  - Astronomy\n",
    "  - Kendrick Lamar Biography\n",
    "\n",
    "I tagli corretti sono alla riga 59/60, alla riga 102/103 e alla riga 181/182.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from gensim.test.utils import simple_preprocess\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funzioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words_ita(phrase):\n",
    "    stop_words = stopwords.words('italian')\n",
    "    phrase = phrase.split()\n",
    "    phrase = [word for word in phrase if word not in stop_words]\n",
    "    return phrase\n",
    "\n",
    "def get_text_from_file(path):\n",
    "    file = []\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    with open (path, 'r') as f:\n",
    "        for row in f:\n",
    "            filtered_s = [w for w in word_tokenize(row) if not w.lower() in stop_words]\n",
    "            file.append(simple_preprocess(str(filtered_s), deacc=True))\n",
    "    f.close()\n",
    "    return file\n",
    "\n",
    "def get_score(text, n_most_words):\n",
    "    '''\n",
    "    Calcolo le n_most_words più frequenti e \n",
    "    restituisco la somma delle loro frequenze \n",
    "    come score del segmento di testo \n",
    "    '''\n",
    "    score = 0\n",
    "    c = Counter()\n",
    "    most_common = []\n",
    "    for row in text:\n",
    "        c.update(row)\n",
    "        \n",
    "    most_common = c.most_common(n_most_words)\n",
    "    \n",
    "    for el in most_common:\n",
    "        score = score + el[1]\n",
    "        \n",
    "    return score\n",
    "\n",
    "def extract_segment(file, start, end):\n",
    "    segment = []\n",
    "    for i in range(int(start), int(end)):\n",
    "        segment.append(file[i])\n",
    "    return segment\n",
    "\n",
    "def get_cuts(ntopic, num_lines):\n",
    "    cut = np.zeros(ntopic)\n",
    "\n",
    "    for k in range(1, ntopic):\n",
    "        cut[k] = random.randint(1, num_lines-1)\n",
    "    \n",
    "    cut = np.append(cut, num_lines)\n",
    "\n",
    "    return sorted(cut)\n",
    "\n",
    "def load_file(file_path):\n",
    "    file = get_text_from_file(file_path)\n",
    "    c = Counter()\n",
    "    num_lines = sum(1 for line in open(file_path)) # Number of lines in the file\n",
    "\n",
    "    for row in file:\n",
    "        c.update(row)\n",
    "        \n",
    "    return num_lines, file, c\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_scores(file, ntopic, n_most_words, n_iteration, num_lines): \n",
    "    scores = np.zeros(ntopic)\n",
    "    sum_scores, max = 0, 0\n",
    "    max_cut = []\n",
    "    \n",
    "    for f in range(n_iteration): \n",
    "\n",
    "        cut = get_cuts(ntopic, num_lines)\n",
    "        \n",
    "        for i in range(len(cut)-1):\n",
    "            text = extract_segment(file, cut[i], cut[i+1])\n",
    "            scores[i] = get_score(text, n_most_words)\n",
    "        \n",
    "        sum_scores = sum(scores)\n",
    "        if(sum_scores > max):\n",
    "            max = sum_scores\n",
    "            max_cut = cut\n",
    "            \n",
    "    return max_cut, max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best cut at lines -->  [0.0, 54.0, 102.0, 180.0, 254.0]\n",
      "the max sum is    -->  361.0\n"
     ]
    }
   ],
   "source": [
    "n_most_words = 3\n",
    "n_topics = 4\n",
    "n_iteration = 10000 \n",
    "c = Counter()\n",
    "file = '../data/segmentation_eng.txt'\n",
    "\n",
    "file_data = load_file(file)\n",
    "\n",
    "num_lines = file_data[0]\n",
    "file = file_data[1]\n",
    "\n",
    "for row in file:\n",
    "    c.update(row)\n",
    "\n",
    "res = max_scores(file, n_topics, n_most_words, n_iteration, num_lines)\n",
    "\n",
    "print(f'\\nBest cut at lines -->  {res[0]}\\nthe max sum is    -->  {res[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisi dei risultati e limiti di questo approccio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problemi dell'approccio utilizzato:\n",
    "\n",
    "- è poco **scalabile** sulla *dimensione del testo*, a cusa dello sforzo computazionale che cresce con il numero di righe del file\n",
    "\n",
    "- la metrica di **similarity** può essere migliorata, impiegando ad esempio una cosine similarity, dopo aver estratto dei vettori numerici dal testo\n",
    "\n",
    "- con un **dataset** con meno varietà negli argomenti le prestazioni potrebbero calare drasticamente\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6847c98a8f86b01c6a19c518cd2f366693b80566b266804d5ca763cbb223f52b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
