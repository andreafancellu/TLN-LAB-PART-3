{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Esercizio 4 - Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approccio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'algoritmo è *brute force* perchè per trovare i tagli corretti li prova tutti generandoli in maniera randomica e associato ad ogni\n",
    "sequenza di tagli un punteggio. Il punteggio è calcolato come la somma degli n termini più frequnti all'interno di ogni segmento.\n",
    "\n",
    "L'idea alla base è che per ogni argomento ci siano un numero di parole che occorrono molte volte, e quindi l'algoritmo andrà a cercare \n",
    "i tagli che vanno a massimizzare questi valori, e che quindi vanno ad isolare i termini più frequenti per ogni segmento.\n",
    "\n",
    "L'algoritmo ha bisogno del numero di tagli in ingresso.\n",
    "\n",
    "Il dataset è stato ripulito rimuovendo tutte le stopwords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*data/segmentation_eng*: Il dataset è costituito da pezzi dei paragrafi di wikipedia di 4 argomenti diversi, in questo caso i 3 argomenti sono:\n",
    "  - Gorillas\n",
    "  - Quantum Computing\n",
    "  - Astronomy\n",
    "  - Kendrick Lamar Biography\n",
    "\n",
    "I tagli corretti sono alla riga 59/60, alla riga 102/103 e alla riga 181/182.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from gensim.test.utils import simple_preprocess\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words_ita(phrase):\n",
    "    stop_words = stopwords.words('italian')\n",
    "    phrase = phrase.split()\n",
    "    phrase = [word for word in phrase if word not in stop_words]\n",
    "    return phrase\n",
    "\n",
    "def get_text_from_file(path):\n",
    "    file = []\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    with open (path, 'r') as f:\n",
    "        for row in f:\n",
    "            filtered_s = [w for w in word_tokenize(row) if not w.lower() in stop_words]\n",
    "            file.append(simple_preprocess(str(filtered_s), deacc=True))\n",
    "    f.close()\n",
    "    return file\n",
    "\n",
    "def frequency(text, n_most_words):\n",
    "    '''\n",
    "    Calcolo le n_most_words più frequenti e \n",
    "    restituisco la somma delle loro frequenze come score del segmento di testo \n",
    "    '''\n",
    "    score = 0\n",
    "    c = Counter()\n",
    "    most_common = []\n",
    "    for row in text:\n",
    "        c.update(row)\n",
    "        \n",
    "    most_common = c.most_common(n_most_words)\n",
    "    # print(most_common)\n",
    "    \n",
    "    for el in most_common:\n",
    "        score = score + el[1]\n",
    "        \n",
    "    return score\n",
    "\n",
    "def extract_segment(file, start, end):\n",
    "    segment = []\n",
    "    for i in range(int(start), int(end)):\n",
    "        segment.append(file[i])\n",
    "    return segment\n",
    "\n",
    "def get_cuts(ntopic, num_lines):\n",
    "    cut = np.zeros(ntopic)\n",
    "\n",
    "    for k in range(1, ntopic):\n",
    "        cut[k] = random.randint(1, num_lines-1)\n",
    "    \n",
    "    cut = np.append(cut, num_lines)\n",
    "\n",
    "    return sorted(cut)\n",
    "\n",
    "def load_file(file_path):\n",
    "    file = get_text_from_file(file_path)\n",
    "    c = Counter()\n",
    "    num_lines = sum(1 for line in open(file_path)) # Number of lines in the file\n",
    "\n",
    "    for row in file:\n",
    "        c.update(row)\n",
    "        \n",
    "    return num_lines, file, c\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_scores(file, ntopic, n_most_words, n_iteration, num_lines): # n_most_words = number of most used words, used in cooccorrence()\n",
    "    scores = np.zeros(ntopic)\n",
    "    sum_scores, max = 0, 0\n",
    "    max_cut = []\n",
    "    \n",
    "    for f in range(n_iteration): \n",
    "        # First generate the random cuts\n",
    "        cut = get_cuts(ntopic, num_lines)\n",
    "        \n",
    "        # Extract the segments by the cuts and calculate the score value for each segment\n",
    "        for i in range(len(cut)-1):\n",
    "            text = extract_segment(file, cut[i], cut[i+1])\n",
    "            scores[i] = frequency(text, n_most_words)\n",
    "        \n",
    "        # Evaluate the result and store the best result\n",
    "        sum_scores = sum(scores)\n",
    "        if(sum_scores > max):\n",
    "            max = sum_scores\n",
    "            max_cut = cut\n",
    "            \n",
    "    return max_cut, max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best cut at lines -->  [0.0, 60.0, 110.0, 183.0, 254.0]\n",
      "the max sum is    -->  357.0\n"
     ]
    }
   ],
   "source": [
    "n_most_words = 3\n",
    "n_topics = 4\n",
    "n_iteration = 10000 # Number of iteration to generate the best result - higher value = more time and better result\n",
    "c = Counter()\n",
    "file = '../data/segmentation_eng.txt'\n",
    "\n",
    "file_data = load_file(file) # Load the file and get the number of lines and the list of most common word in the file\n",
    "\n",
    "num_lines = file_data[0]\n",
    "file = file_data[1]\n",
    "\n",
    "for row in file:\n",
    "    c.update(row)\n",
    "\n",
    "res = max_scores(file, n_topics, n_most_words, n_iteration, num_lines)\n",
    "\n",
    "print(f'\\nBest cut at lines -->  {res[0]}\\nthe max sum is    -->  {res[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisi dei risultati e limiti di questo approccio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per quanto semplice come approccio riesce a trovare i tagli corretti in poche esecuzioni sul nostro dataset, ma presenta diversi problemi:\n",
    "\n",
    "- **scala male su testi grossi**: Purtroppo l'algoritmo per come è stato studiato scala male su dataset più grossi perchè deve esaminare\n",
    "  più righe di testo e deve provare più combinazioni di taglie essendoci più righe ed eventualmente più tagli.\n",
    "\n",
    "- **similarity**: Per valutare la similarity all'interno di un documento prendiamo la somma dei termini più frequenti all'interno di ogni segmento.\n",
    "  Un possibile miglioramento è quello di trasformare il testo in un vettore e calcolare la cosine similarity del segmento e si va ad effettuare \n",
    "  il taglio dove la similarity va a decrescere per poi risalire, come abbiamo visto a lezione. Abbiamo deciso di non seguire questa strada per \n",
    "  trovare un approccio alternativo, anche se abbiamo ottenuto performance peggiori sia in termini di tempi di esecuzione che di precisione dei tagli.\n",
    "\n",
    "- **dataset**: L'algoritmo performa bene su questo testo perchè in ogni segmento è presente una grande quantità di termini \"unici\", o meglio \n",
    "  che compaiono solamente all'interno di quel segmento. Se si tenta di applicare questo algoritmo a un dataset meno eterogeneo è probabile\n",
    "  che i tagli siano più imprecisi.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('tln')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ab10897505b9f9d030457e3ccdb0b9a6182530c85f1abe0495a5530d8bfa2502"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
